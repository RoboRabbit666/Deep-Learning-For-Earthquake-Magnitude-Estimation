{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Title: Original MagNet Implementation for Single-Station Magnitude Estimation\n",
    "\n",
    "This notebook implements the original MagNet architecture (Mousavi & Beroza, 2020) \n",
    "for earthquake magnitude estimation, including:\n",
    "1. Model architecture implementation\n",
    "2. Custom loss function with uncertainty quantification \n",
    "3. Training and evaluation pipeline\n",
    "4. Performance analysis and visualization\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 1: Setup and Imports \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 2: Dataset Class\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Dataset class for earthquake waveforms and magnitude labels.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 3: Model Architecture\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeModel(nn.Module):\n",
    "    \"\"\"Original MagNet architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(4, padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(32, 100, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(200, 2)  # Magnitude and log variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 4: Training Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, run_id=None, \n",
    "                 test_seed=None, model_seed=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.run_id = run_id\n",
    "        self.test_seed = test_seed\n",
    "        self.model_seed = model_seed\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "        model_filename = f'best_model_Run_{self.run_id}_test_seed_{self.test_seed}_model_seed_{self.model_seed}.pth'\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \"\"\"Custom loss function combining prediction error and uncertainty.\"\"\"\n",
    "    y_hat = y_pred[:, 0]\n",
    "    s = y_pred[:, 1]\n",
    "    return torch.mean(0.5 * torch.exp(-s) * (y_true - y_hat)**2 + 0.5 * s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 5: Training Function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=300, patience=5,\n",
    "                run_id=None, test_seed=None, model_seed=None):\n",
    "    \"\"\"Main training function with early stopping and LR scheduling.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=np.sqrt(0.1),\n",
    "        cooldown=0, patience=4, verbose=True, min_lr=0.5e-6\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, verbose=True,\n",
    "        run_id=run_id, test_seed=test_seed, model_seed=model_seed\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = custom_loss(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = custom_loss(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        val_loss /= len(val_loader)\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        # Update LR and check early stopping\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Training Loss: {running_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        train_losses.append(running_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 6: Uncertainty Estimation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def estimate_uncertainty(model, data_loader, num_samples=50):\n",
    "    \"\"\"Estimate aleatoric and epistemic uncertainties using MC dropout.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Enable dropout at test time\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "    \n",
    "    predictions = []\n",
    "    log_variances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            batch_predictions = []\n",
    "            batch_log_variances = []\n",
    "            for data, _ in data_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                batch_predictions.append(output[:, 0].cpu().numpy())\n",
    "                batch_log_variances.append(output[:, 1].cpu().numpy())\n",
    "            predictions.append(np.concatenate(batch_predictions))\n",
    "            log_variances.append(np.concatenate(batch_log_variances))\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    log_variances = np.array(log_variances)\n",
    "    \n",
    "    # Calculate different types of uncertainty\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    yhat_squared_mean = np.mean(np.square(predictions), axis=0)\n",
    "    \n",
    "    sigma_squared = np.power(10, log_variances)\n",
    "    aleatoric_uncertainty = np.mean(sigma_squared, axis=0)\n",
    "    \n",
    "    epistemic_uncertainty = np.std(predictions, axis=0)\n",
    "    combined_uncertainty = yhat_squared_mean - np.square(mean_prediction) + aleatoric_uncertainty\n",
    "    \n",
    "    return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 7: Evaluation Function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model(model, test_loader, run_id, test_seed, model_seed):\n",
    "    \"\"\"Evaluate model performance with uncertainty estimation.\"\"\"\n",
    "    model_filename = f'best_model_Run_{run_id}_test_seed_{test_seed}_model_seed_{model_seed}.pth'\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    \n",
    "    mean_pred, epistemic_unc, aleatoric_unc, combined_unc = estimate_uncertainty(model, test_loader)\n",
    "    \n",
    "    true_values = []\n",
    "    for _, target in test_loader:\n",
    "        true_values.append(target.numpy())\n",
    "    true_values = np.concatenate(true_values)\n",
    "    \n",
    "    mae = np.mean(np.abs(mean_pred - true_values))\n",
    "    \n",
    "    return mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 8: Visualization Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss Curves', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_scatter(true_values, predictions, mae):\n",
    "    \"\"\"Plot scatter plot of predicted vs true magnitudes.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(true_values, predictions, alpha=0.5)\n",
    "    plt.plot([min(true_values), max(true_values)], \n",
    "             [min(true_values), max(true_values)], 'r--')\n",
    "    plt.xlabel('True Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Prediction vs Ground Truth (MAE: {mae:.3f})', \n",
    "             fontsize=16, fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_uncertainty_analysis(predictions, aleatoric_unc, epistemic_unc, combined_unc):\n",
    "    \"\"\"Plot uncertainty analysis visualizations.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Aleatoric uncertainty\n",
    "    axes[0].scatter(predictions, aleatoric_unc, alpha=0.5)\n",
    "    axes[0].set_xlabel('Predicted Magnitude', fontsize=12)\n",
    "    axes[0].set_ylabel('Aleatoric Uncertainty', fontsize=12)\n",
    "    axes[0].set_title('Aleatoric Uncertainty vs Predictions', fontsize=14)\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Epistemic uncertainty\n",
    "    axes[1].scatter(predictions, epistemic_unc, alpha=0.5)\n",
    "    axes[1].set_xlabel('Predicted Magnitude', fontsize=12)\n",
    "    axes[1].set_ylabel('Epistemic Uncertainty', fontsize=12)\n",
    "    axes[1].set_title('Epistemic Uncertainty vs Predictions', fontsize=14)\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Combined uncertainty\n",
    "    axes[2].scatter(predictions, combined_unc, alpha=0.5)\n",
    "    axes[2].set_xlabel('Predicted Magnitude', fontsize=12)\n",
    "    axes[2].set_ylabel('Combined Uncertainty', fontsize=12)\n",
    "    axes[2].set_title('Combined Uncertainty vs Predictions', fontsize=14)\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 9: Main Execution\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    data_file = \"pre_processed_data.npy\" # Change to your file path\n",
    "    labels_file = \"pre_processed_labels.npy\" # Change to your file path\n",
    "    \n",
    "    # Verify files exist\n",
    "    assert os.path.isfile(data_file), f\"Data file not found at {data_file}\"\n",
    "    assert os.path.isfile(labels_file), f\"Labels file not found at {labels_file}\"\n",
    "    \n",
    "    # Load data\n",
    "    data = torch.tensor(np.load(data_file), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.load(labels_file), dtype=torch.float32)\n",
    "    print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Model seeds for multiple runs\n",
    "    model_seeds = [42, 123, 256, 789, 1024]\n",
    "    results = []\n",
    "    \n",
    "    # Run experiments\n",
    "    for run_id in tqdm(range(1, 11)):  # 10 different test sets\n",
    "        test_results = []\n",
    "        for model_seed in model_seeds:\n",
    "            # Split data\n",
    "            train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "                data, labels, test_size=0.2, shuffle=True)\n",
    "            train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "                train_val_data, train_val_labels, \n",
    "                test_size=0.125, shuffle=True)\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = EarthquakeDataset(train_data, train_labels)\n",
    "            val_dataset = EarthquakeDataset(val_data, val_labels)\n",
    "            test_dataset = EarthquakeDataset(test_data, test_labels)\n",
    "            \n",
    "            # Create dataloaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=256, \n",
    "                                    shuffle=True, num_workers=2)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=256, \n",
    "                                  shuffle=False, num_workers=2)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=256, \n",
    "                                   shuffle=False, num_workers=2)\n",
    "            \n",
    "            # Initialize model\n",
    "            model = EarthquakeModel().to(device)\n",
    "            print(summary(model, input_size=(256, 3000, 3)))\n",
    "            \n",
    "            # Train model\n",
    "            train_losses, val_losses = train_model(\n",
    "                model, train_loader, val_loader,\n",
    "                run_id=run_id, test_seed=run_id, model_seed=model_seed\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc = \\\n",
    "                evaluate_model(model, test_loader, run_id, run_id, model_seed)\n",
    "            \n",
    "            result = {\n",
    "                \"test_seed\": run_id,\n",
    "                \"model_seed\": model_seed,\n",
    "                \"mae\": float(mae),\n",
    "                \"mean_aleatoric_uncertainty\": float(np.mean(aleatoric_unc)),\n",
    "                \"mean_epistemic_uncertainty\": float(np.mean(epistemic_unc)),\n",
    "                \"mean_combined_uncertainty\": float(np.mean(combined_unc))\n",
    "            }\n",
    "            \n",
    "            test_results.append(result)\n",
    "            \n",
    "            # Plot results for this run\n",
    "            plot_training_curves(train_losses, val_losses)\n",
    "            plot_prediction_scatter(true_values, mean_pred, mae)\n",
    "            plot_uncertainty_analysis(mean_pred, aleatoric_unc, \n",
    "                                   epistemic_unc, combined_unc)\n",
    "            \n",
    "            print(f\"\\nResults for Run {run_id}, Model Seed {model_seed}:\")\n",
    "            print(f\"MAE: {mae:.4f}\")\n",
    "            print(f\"Mean Aleatoric Uncertainty: {np.mean(aleatoric_unc):.4f}\")\n",
    "            print(f\"Mean Epistemic Uncertainty: {np.mean(epistemic_unc):.4f}\")\n",
    "            print(f\"Mean Combined Uncertainty: {np.mean(combined_unc):.4f}\")\n",
    "        \n",
    "        # Find median performance for this test set\n",
    "        sorted_results = sorted(test_results, key=lambda x: x['mae'])\n",
    "        median_result = sorted_results[2]  # Index 2 is the median of 5\n",
    "        \n",
    "        results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"median_mae\": median_result['mae'],\n",
    "            \"median_aleatoric_uncertainty\": median_result['mean_aleatoric_uncertainty'],\n",
    "            \"median_epistemic_uncertainty\": median_result['mean_epistemic_uncertainty'],\n",
    "            \"median_combined_uncertainty\": median_result['mean_combined_uncertainty'],\n",
    "            \"all_results\": test_results\n",
    "        })\n",
    "        \n",
    "        # Save results after each test seed\n",
    "        with open('baseline_magnet_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    maes = [result[\"median_mae\"] for result in results]\n",
    "    mean_mae = np.mean(maes)\n",
    "    std_mae = np.std(maes)\n",
    "    \n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(f\"Mean MAE: {mean_mae:.4f}\")\n",
    "    print(f\"Standard Deviation of MAE: {std_mae:.4f}\")\n",
    "    \n",
    "    # Plot MAE distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(maes, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(mean_mae, color='red', linestyle='dashed', linewidth=2,\n",
    "                label=f'Mean MAE = {mean_mae:.4f}')\n",
    "    plt.axvline(mean_mae + std_mae, color='green', linestyle='dotted', linewidth=2,\n",
    "                label=f'Mean Â± Std = {mean_mae + std_mae:.4f}')\n",
    "    plt.axvline(mean_mae - std_mae, color='green', linestyle='dotted', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('MAE', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "    plt.title('Distribution of Median MAEs over Different Test Sets',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlations between uncertainties and errors\n",
    "    errors = np.abs(mean_pred - true_values)\n",
    "    aleatoric_corr = np.corrcoef(errors, aleatoric_unc)[0,1]\n",
    "    epistemic_corr = np.corrcoef(errors, epistemic_unc)[0,1]\n",
    "    combined_corr = np.corrcoef(errors, combined_unc)[0,1]\n",
    "    \n",
    "    print(\"\\nCorrelations between Uncertainties and Absolute Errors:\")\n",
    "    print(f\"Aleatoric Uncertainty: {aleatoric_corr:.4f}\")\n",
    "    print(f\"Epistemic Uncertainty: {epistemic_corr:.4f}\")\n",
    "    print(f\"Combined Uncertainty: {combined_corr:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = 'final_baseline_model.pth'\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
